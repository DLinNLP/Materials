{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tagger_Multi-task.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLImtw4oo0WV",
        "colab_type": "text"
      },
      "source": [
        "# Multi-task Learning\n",
        "---\n",
        "\n",
        "<font size=\"4\"> \n",
        "  \n",
        "  Multi-task learning is an approach for jointly training multiple models. It can be simply implemented in neural networks by learning tasks in parallel while using a shared representation.\n",
        "  \n",
        "The simplest way is when we have different outputs for the same input and simultaneously train a model to predict the two or more outputs. \n",
        "  \n",
        "Parallel models can have both shared and independent layers.\n",
        "  \n",
        "  <figure>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1LdwHoHlwm2wQj6SHUazYkNcavTCJp1_X\" width=\"400\" height=\"300\"/>\n",
        "<figcaption>Multi-task learning</figcaption></center>\n",
        "</figure>\n",
        "  \n",
        "In this excercise, we use the tags for chunking (which is provided in CONLL 2003 dataset in the third column) as our auxiliary outputs. The idea is that these two tasks can benefit from each other.\n",
        "  \n",
        "  \n",
        "The sections of the code that should be modified for multi-tasking are as follows:\n",
        "  \n",
        "\n",
        "\n",
        "* Modify `readfile()`, to read the chunking column as well\n",
        "* Create a dictionary that encode auxiliary tags\n",
        "* Return auxiliary tags from class `CoNLL2003NER(Dataset)`\n",
        "* Modify the model class. The architecture of the model will consist of one LSTM layer on top of embedding, then, two parallel LSTM layers, one to learn auxiliary tags and one to learn the main tags, and finally two Linear layers in similar fashion. The model returns two outputs accordingly.\n",
        "* In the training loop, two losses will be computed, one for main tags predictions and one for auxiliary tags prediction. Backpropagation would be performed based on the addition of the two losses.\n",
        " \n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF8OUxU6jSkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following two lines authorises access to Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJnwvlt7jffE",
        "colab_type": "code",
        "outputId": "de3effb2-a220-48e5-8bfd-b47a05839f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pdb\n",
        "import torch\n",
        "import gensim\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"cuda device {}available\".format(\"\" if use_cuda else \"un\"))\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Parameters\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 200\n",
        "BATCH_SIZE = 256 #300\n",
        "EPOCHS = 30\n",
        "LSTM_DROPOUT = 0.3\n",
        "USE_PRETRAINED = False\n",
        "\n",
        "# data dir\n",
        "data_folder = \"/content/drive/My Drive/DLinNLP/Data\"\n",
        "# embeddings folder \n",
        "embed_folder = \"/content/drive/My Drive/DLinNLP/embeddings\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda device available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GWyxC9mu32c",
        "colab_type": "text"
      },
      "source": [
        "## Data Format\n",
        "---\n",
        "<font size=\"4\"> One standard file format for representing IOB-annotated datasets is called CONLL. \n",
        "  \n",
        "A CONLL file contains one token per line and an empty line indicating the end of a sentence. Each token may be annotated by several tab-separated columns indicating information about the token (e.g. token raw form) or differrent tags assigned to it (e.g. syntactic and morphological labels).\n",
        "</font>\n",
        "\n",
        "<P>\n",
        "<table align=\"left\" style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td>Welsh</td>\n",
        "    <td>NNP</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>B-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>National</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Farmers</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>'</td>\n",
        "    <td>POS</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Union</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(</td>\n",
        "    <td>(</td> \n",
        "    <td>O</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NFU</td>\n",
        "    <td>NNP</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>B-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>)</td>\n",
        "    <td>)</td> \n",
        "    <td>O</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>chairman</td>\n",
        "    <td>NN</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>John</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>B-PER</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Lloyd</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-PER</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>said</td>\n",
        "    <td>VBD</td> \n",
        "    <td>B-VP</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>on</td>\n",
        "    <td>IN</td> \n",
        "    <td>B-PP</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>BBC</td>\n",
        "    <td>NNP</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>B-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>radio</td>\n",
        "    <td>NN</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>.</td>\n",
        "    <td>.</td> \n",
        "    <td>O</td>\n",
        "    <td><b>O</b></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><br></tr>\n",
        "    <td><font size=\"4\"> Here, we read the CONLL 2003 dataset! <font></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "<br>\n",
        "</P> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpafGCXojfcB",
        "colab_type": "code",
        "outputId": "61501501-3408-406f-8623-cfc41643769a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#\n",
        "# This fuction has been modified to read the chunking column as well.\n",
        "\n",
        "def readfile(filename):\n",
        "    f = open(filename)\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in f:\n",
        "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
        "            if len(sentence) > 0:\n",
        "                sentences.append(sentence)\n",
        "                sentence = []\n",
        "            continue\n",
        "        splits = line.split(' ')\n",
        "        sentence.append([splits[0].strip(), splits[-2].strip(), splits[-1].strip()])\n",
        "\n",
        "    if len(sentence) > 0:\n",
        "        sentences.append(sentence)\n",
        "        sentence = []\n",
        "    sentences = [tuple(zip(*l)) for l in sentences]\n",
        "    return sentences\n",
        "\n",
        "train_data = np.array(readfile(data_folder+'/train.txt'))\n",
        "dev_data = np.array(readfile(data_folder+'/dev.txt'))\n",
        "test_data = np.array(readfile(data_folder+'/test.txt'))\n",
        "\n",
        "#train_data = train_data[0:6000,:]\n",
        "print(train_data.shape)\n",
        "print(train_data[0])\n",
        "print(dev_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14041, 3)\n",
            "[('EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.')\n",
            " ('B-NP', 'B-VP', 'B-NP', 'I-NP', 'B-VP', 'I-VP', 'B-NP', 'I-NP', 'O')\n",
            " ('B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O')]\n",
            "(3250, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXW2CYLTA8zf",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing \n",
        "---\n",
        "\n",
        "<font size=\"4\">It is common to sort data instances based on their lengths. This way, the lengths of sequences in each batch would be more homogeneous. \n",
        "<font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL_zgRz6jfYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This reordering is useful for padding\n",
        "def tensor_reorder(data):\n",
        "    \"\"\"reorders tensors from longest to shortest\"\"\"\n",
        "    lengths = [len(i[0]) for i in data]\n",
        "    max_len = max(lengths)\n",
        "    lengths = torch.LongTensor(lengths)\n",
        "    lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "    data = data[perm_idx]\n",
        "    return data\n",
        "\n",
        "train_data = tensor_reorder(train_data)\n",
        "dev_data = tensor_reorder(dev_data)\n",
        "\n",
        "test_data = tensor_reorder(test_data)\n",
        "\n",
        "MAX_LEN = max(len(train_data[0][0]), len(dev_data[0][0])) # we set the maximum length from the max seq in train "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wMKVnTrjfWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here, we create a dictionary that maps all words to indices (for encoding)\n",
        "\n",
        "all_words = list(set([w for sent in np.concatenate((train_data,dev_data), axis=0) for w in sent[0]]))\n",
        "\n",
        "word_to_ix = {t:i+2 for i, t in enumerate(all_words)}\n",
        "word_to_ix['<PAD>'] = 0\n",
        "word_to_ix['<UNK>'] = 1\n",
        "\n",
        "\n",
        "all_tags_aux = list(set([tag for sent in np.concatenate((train_data,dev_data), axis=0) for tag in sent[1]]))\n",
        "tag_to_ix_aux = {t:i+1 for i, t in enumerate(all_tags_aux)}\n",
        "tag_to_ix_aux['<PAD>'] = 0\n",
        "ix_to_tag_aux = {v: k for k, v in tag_to_ix_aux.items()}\n",
        "\n",
        "# The tagset is simplified (NE categories not included) \n",
        "tag_to_ix = {'<PAD>':0, 'B-MISC':1, 'B-LOC':1, 'B-ORG':1, 'B-PER':1,\n",
        "             'I-MISC':2, 'I-PER':2, 'I-ORG':2, 'I-LOC':2, 'O':3}\n",
        "ix_to_tag = {0:'<PAD>', 1:'B', 2:'I', 3:'O'}\n",
        "\n",
        "## If we wanted to consider tags in their entirety:\n",
        "# all_tags = list(set([tag for sent_tag in train_data for tag in sent_tag[1]]))\n",
        "# tag_to_ix = {t:i+1 for i, t in enumerate(all_tags)}\n",
        "# tag_to_ix['<PAD>'] = 0\n",
        "# ix_to_tag = {v: k for k, v in tag_to_ix.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ5P-gPUjhRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class creates a generator of data batches in order to faster the access to data.\n",
        "class CoNLL2003NER(Dataset):\n",
        "\n",
        "    def __init__(self, X, max_len, word_to_ix, tag_to_ix_aux, tag_to_ix):\n",
        "        self.X = X\n",
        "        self.max_len = max_len\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix_aux = tag_to_ix_aux\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        \n",
        "    def transform(self, seq, to_ix):\n",
        "        idxs = [to_ix[w] if w in to_ix else 1 for w in seq]\n",
        "        if len(idxs) > self.max_len:\n",
        "            # Truncating\n",
        "            idxs = idxs[:self.max_len]    \n",
        "        else:\n",
        "            # Padding\n",
        "            idxs += [0]*(self.max_len-len(seq))\n",
        "        \n",
        "        return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.transform(self.X[idx][0],word_to_ix), self.transform(self.X[idx][1], tag_to_ix_aux), self.transform(self.X[idx][2], tag_to_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6kv5mDemXfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'batch_size': BATCH_SIZE,\n",
        "          'shuffle': False,\n",
        "          'num_workers': 6,\n",
        "          'drop_last':True}\n",
        "\n",
        "\n",
        "train_data = CoNLL2003NER(train_data, MAX_LEN, word_to_ix, tag_to_ix_aux, tag_to_ix)\n",
        "train_data_generator = DataLoader(train_data, **params)\n",
        "\n",
        "test_data = CoNLL2003NER(test_data, MAX_LEN, word_to_ix, tag_to_ix_aux, tag_to_ix)\n",
        "test_data_generator = DataLoader(test_data, **params)\n",
        "\n",
        "dev_data = CoNLL2003NER(dev_data, MAX_LEN, word_to_ix, tag_to_ix_aux, tag_to_ix)\n",
        "dev_data_generator = DataLoader(dev_data, **params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P8G0y54rpr8",
        "colab_type": "text"
      },
      "source": [
        "<font size=\"4\"> As you remember, in PyTorch, we build a class that inherits from Pytorch's nn.Module and includes two critical functions: <i>\\_\\_init\\_\\_</i> and <i>forward</i>.\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gm5mWQlmXbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An LSTM_based Tgger for Multi-task learning\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, aux_tagset_size, tagset_size, max_len):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
        "        \n",
        "        ### TO DO ####   (10 min)\n",
        "        # load weights from pre-trained vectors or with random initialization \n",
        "        \n",
        "        \n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=LSTM_DROPOUT, bidirectional=True, num_layers=1)\n",
        "        self.lstm2a = nn.LSTM(2 * hidden_dim, hidden_dim, batch_first=True, dropout=LSTM_DROPOUT, bidirectional=True, num_layers=1)\n",
        "        self.lstm2b = nn.LSTM(2 * hidden_dim, hidden_dim, batch_first=True, dropout=LSTM_DROPOUT, bidirectional=True, num_layers=1)\n",
        "        ### TO DO ####\n",
        "        ### Add other layers to the Model ###\n",
        "\n",
        "        \n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2aux_tag = nn.Linear(2*hidden_dim, aux_tagset_size)\n",
        "        self.hidden2tag = nn.Linear(2*hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence): \n",
        "        embeds = self.word_embeddings(sentence)     # SHAPE: [BATCH_SIZE,MAX_LEN,WORD_EMBEDDING_DIM]\n",
        "        \n",
        "        ### TO DO ####   (10 min)\n",
        "        # Add packed padded sequences to help the loss ignore computation for PAD-labeld parts in each sequence\n",
        "        lstm_shared, _ = self.lstm1(embeds)\n",
        "        \n",
        "        lstm_out_aux, _ = self.lstm2a(lstm_shared)\n",
        "        aux_tag_space = self.hidden2aux_tag(lstm_out_aux)\n",
        "        aux_tag_scores = F.log_softmax(aux_tag_space, dim=1) \n",
        "        \n",
        "        lstm_out, _ = self.lstm2b(lstm_shared)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1) \n",
        "        \n",
        "        return aux_tag_scores, tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBoxn3i6mXX_",
        "colab_type": "code",
        "outputId": "d9bd32a0-7ccf-4713-be84-ab2606af9046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(ix_to_tag_aux), len(ix_to_tag), MAX_LEN).to(device)\n",
        "\n",
        "# The negative log likelihood loss. \n",
        "# It is useful to train a classification problem with C classes.\n",
        "loss_function = nn.NLLLoss(ignore_index=0)  \n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001) \n",
        "\n",
        "print(model)\n",
        "print(model.parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMTagger(\n",
            "  (word_embeddings): Embedding(26885, 300)\n",
            "  (lstm1): LSTM(300, 200, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (lstm2a): LSTM(400, 200, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (lstm2b): LSTM(400, 200, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (hidden2aux_tag): Linear(in_features=400, out_features=22, bias=True)\n",
            "  (hidden2tag): Linear(in_features=400, out_features=4, bias=True)\n",
            ")\n",
            "<bound method Module.parameters of LSTMTagger(\n",
            "  (word_embeddings): Embedding(26885, 300)\n",
            "  (lstm1): LSTM(300, 200, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (lstm2a): LSTM(400, 200, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (lstm2b): LSTM(400, 200, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (hidden2aux_tag): Linear(in_features=400, out_features=22, bias=True)\n",
            "  (hidden2tag): Linear(in_features=400, out_features=4, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPfxkFwB3Ngk",
        "colab_type": "code",
        "outputId": "12d62704-c014-408e-f05f-4319f1310368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check the number of parameters\n",
        "pp=0\n",
        "for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "print(pp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10805526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PetKGeMUmXUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oG8FVrcmXMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Write an early stopping procedure\n",
        "# Write an early stopping procedure\n",
        "\n",
        "def early_stop(losses, patience):\n",
        "    \"\"\"stop execution if there is consecutive decline/stagnation in the loss values.\n",
        "       patience determines how quickly we take action. \n",
        "    \"\"\"\n",
        "    stop = False\n",
        "    patience += 1\n",
        "    if len(losses)>patience and min(losses[-patience:])==losses[-patience]:\n",
        "        stop = True\n",
        "    return stop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnB2nxhJmXC_",
        "colab_type": "code",
        "outputId": "57db9559-fb0b-4017-fe24-d968e2b0a595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "def trainer(model, epochs):\n",
        "    \"\"\"train the model for the specified # of epochs\"\"\"\n",
        "    \n",
        "    \n",
        "    avg_train_losses = []\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "                ################\n",
        "                ## train mode ##\n",
        "                ################\n",
        "        model.train() # set the model to training mode  \n",
        "        print('Epoch:', epoch+1)\n",
        "        t0 = time.time()\n",
        "        n_correct, n_total = 0, 0\n",
        "         \n",
        "        batch_losses = []\n",
        "        valid_losses = []\n",
        "        for sentences, aux_tags, tags in train_data_generator:\n",
        "            sentences, tags = sentences.to(device), tags.to(device)\n",
        "            aux_tags = aux_tags.to(device)\n",
        "            \n",
        "            # clear gradients \n",
        "            model.zero_grad()\n",
        "\n",
        "            # Run forward pass\n",
        "            predictions_aux, predictions = model(sentences)\n",
        "            \n",
        "            # compute the loss, gradients, and update the parameters\n",
        "            predictions = predictions.permute(0,2,1)       # loss presumes labels to come 2nd (hence the permute)\n",
        "            predictions_aux = predictions_aux.permute(0,2,1)       # loss presumes labels to come 2nd (hence the permute)\n",
        "            \n",
        "            batch_loss_prim = loss_function(predictions, tags)  # This computes average loss over all instances of the batch \n",
        "            batch_loss_aux = loss_function(predictions_aux, aux_tags)\n",
        "            \n",
        "            batch_loss = batch_loss_prim + batch_loss_aux\n",
        "            #print(batch_loss)\n",
        "            \n",
        "            batch_losses.append(batch_loss_prim.item())\n",
        "            \n",
        "            # compute number of correct predictions per epoch   \n",
        "            outputs = torch.argmax(predictions, dim=1)        \n",
        "            \n",
        "            n_correct += torch.sum(outputs==tags, dtype=torch.float)\n",
        "            n_total += float(tags.size(0) * tags.size(1))  # denominator: batch_size * max_len (e.g. 100 * 52)\n",
        "            \n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        epoch_acc = n_correct/n_total\n",
        "        epoch_loss = np.average(batch_losses)\n",
        "        avg_train_losses.append(epoch_loss) # for keeping track of avg train losses\n",
        "  \n",
        "                ################\n",
        "                ## eval mode ###\n",
        "                ################\n",
        "        model.eval() # set the model to eval mode\n",
        "        for valid_sentences, valid_tags_aux, valid_tags in dev_data_generator:\n",
        "            valid_sentences, valid_tags_aux, valid_tags = valid_sentences.to(device), valid_tags_aux.to(device), valid_tags.to(device)\n",
        "            \n",
        "            # Run forward pass. Note since we are in eval mode, we don't need to set grad to zero  \n",
        "            _, valid_predictions = model(valid_sentences)\n",
        "            valid_predictions = valid_predictions.permute(0,2,1)\n",
        "            # calculate the average loss \n",
        "            valid_batch_loss = loss_function(valid_predictions, valid_tags)\n",
        "            valid_losses.append(valid_batch_loss.item())\n",
        "            \n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        t = time.time()\n",
        "        print('epoch loss: {}\\tepoch acc: {}\\tvalid loss:{}\\ttime:{}'.format(epoch_loss, epoch_acc.item(), valid_loss, t-t0))\n",
        "        \n",
        "        ### TO DO ###    (10 min)\n",
        "        # Early Stopping\n",
        "        # end training if validation losses stagnate/increase \n",
        "        if early_stop(avg_valid_losses, patience=5):\n",
        "            print(\"Early stopping...\")\n",
        "            break\n",
        "        \n",
        "        ### TO DO ###    (5 min)\n",
        "        # Saving the best Model\n",
        "        \n",
        "    return model, avg_train_losses, avg_valid_losses \n",
        "            \n",
        "model, avg_train_losses, avg_valid_losses = trainer(model, EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "epoch loss: 2.3598830473643764\tepoch acc: 0.9187389016151428\tvalid loss:2.4531965057055154\ttime:25.810060739517212\n",
            "Epoch: 2\n",
            "epoch loss: 2.1591251440070294\tepoch acc: 0.937457799911499\tvalid loss:2.3613521506388984\ttime:25.57040047645569\n",
            "Epoch: 3\n",
            "epoch loss: 2.07207213424974\tepoch acc: 0.9566561579704285\tvalid loss:2.246644283334414\ttime:25.690938472747803\n",
            "Epoch: 4\n",
            "epoch loss: 2.00152274152195\tepoch acc: 0.9674172401428223\tvalid loss:2.201429451505343\ttime:25.553412914276123\n",
            "Epoch: 5\n",
            "epoch loss: 1.9562687065314364\tepoch acc: 0.9739686250686646\tvalid loss:2.189422751466433\ttime:25.570494651794434\n",
            "Epoch: 6\n",
            "epoch loss: 1.925710806278167\tepoch acc: 0.977774977684021\tvalid loss:2.188921516140302\ttime:25.56201171875\n",
            "Epoch: 7\n",
            "epoch loss: 1.9068788560452286\tepoch acc: 0.9805616140365601\tvalid loss:2.17028546333313\ttime:25.62813949584961\n",
            "Epoch: 8\n",
            "epoch loss: 1.8933848789720624\tepoch acc: 0.9825006723403931\tvalid loss:2.1612362762292228\ttime:25.49254608154297\n",
            "Epoch: 9\n",
            "epoch loss: 1.8850758848366913\tepoch acc: 0.983944833278656\tvalid loss:2.1604034105936685\ttime:25.578016996383667\n",
            "Epoch: 10\n",
            "epoch loss: 1.8808186760655157\tepoch acc: 0.9845600128173828\tvalid loss:2.151881923278173\ttime:25.482452392578125\n",
            "Epoch: 11\n",
            "epoch loss: 1.877629781624785\tepoch acc: 0.9853308200836182\tvalid loss:2.162857453028361\ttime:25.567861318588257\n",
            "Epoch: 12\n",
            "epoch loss: 1.8757383294955448\tepoch acc: 0.9856016039848328\tvalid loss:2.1629972656567893\ttime:25.625568628311157\n",
            "Epoch: 13\n",
            "epoch loss: 1.8730536950407204\tepoch acc: 0.9854806065559387\tvalid loss:2.174350599447886\ttime:25.578121662139893\n",
            "Epoch: 14\n",
            "epoch loss: 1.871016371857237\tepoch acc: 0.985467791557312\tvalid loss:2.1606350938479104\ttime:25.60845184326172\n",
            "Epoch: 15\n",
            "epoch loss: 1.8693392519597654\tepoch acc: 0.9857680201530457\tvalid loss:2.166117856899897\ttime:25.642690658569336\n",
            "Early stopping...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa4vzoEReMeB",
        "colab_type": "code",
        "outputId": "b34db08e-4385-4e9c-ac39-1937891f0d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        }
      },
      "source": [
        "# Visualizing the loss as the network trained\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.plot(range(1,len(avg_train_losses)+1),avg_train_losses, label='Training Loss')\n",
        "plt.plot(range(1,len(avg_valid_losses)+1),avg_valid_losses,label='Validation Loss')\n",
        "\n",
        "#plt.ylim(1.8, 2.2)\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = avg_valid_losses.index(min(avg_valid_losses))+1 \n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAHjCAYAAACNTANBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9XRK1GJSjBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define evaluation metrics (acc, precision, recall, and f1-score)\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "class Evaluate():\n",
        "    def __init__(self, out, labels):\n",
        "        self.out = out.numpy().flatten()\n",
        "        self.labels = labels.numpy().flatten()\n",
        "    def accuracy(self):\n",
        "        nb_correct = sum(y_t==y_p for y_t, y_p in zip(self.labels, self.out))\n",
        "        nb_true = len(self.labels)\n",
        "        score = nb_correct / nb_true\n",
        "        return score\n",
        "    def precision_recall_fscore(self, tag_list, average='macro'):\n",
        "        return precision_recall_fscore_support(self.labels, self.out, \n",
        "                                                  average=average,labels=tag_list)[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tWDZVOimW6L",
        "colab_type": "code",
        "outputId": "c7aa06ee-05ec-4741-a913-4c38443c36fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# evaluate the model on test set \n",
        "\n",
        "all_preds = torch.LongTensor().to(device)\n",
        "all_labels = torch.LongTensor().to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sent, aux_target, target in test_data_generator:\n",
        "        sent, target = sent.to(device), target.to(device)\n",
        "        _, tag_scores = model(sent)\n",
        "        \n",
        "        predict = tag_scores.data.max(2, keepdim=True)[1]        \n",
        "        all_preds = torch.cat([all_preds, predict])\n",
        "        all_labels = torch.cat([all_labels,target])\n",
        "    \n",
        "    all_preds, all_labels = all_preds.cpu(), all_labels.cpu()\n",
        "    all_preds = all_preds.squeeze(dim=-1)\n",
        "    print()\n",
        "    evaluator = Evaluate(all_preds, all_labels)\n",
        "    print('Overall Results on the Test set:')\n",
        "    print('Accuracy\\t{}'.format(evaluator.accuracy()))\n",
        "    pr, rc, fm = evaluator.precision_recall_fscore(tag_list=[1,2,3]) # we ignore pad  \n",
        "    print('Precision\\t{}\\nRecall\\t\\t{}\\nF1-score\\t{}'.format(pr,rc,fm))\n",
        "    \n",
        "    print('\\n==================\\n')\n",
        "    \n",
        "    print('# Results ignoring O:\\n')\n",
        "    tag_list = [1,2] # we ignore both pad and O\n",
        "    pr, rc, fm = evaluator.precision_recall_fscore(tag_list)\n",
        "    print('Precision\\t{}\\nRecall\\t\\t{}\\nF1-score\\t{}'.format(pr,rc,fm))         "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Overall Results on the Test set:\n",
            "Accuracy\t0.980694775357386\n",
            "Precision\t0.6624950150343248\n",
            "Recall\t\t0.8473748813424793\n",
            "F1-score\t0.7135696859832068\n",
            "\n",
            "==================\n",
            "\n",
            "# Results ignoring O:\n",
            "\n",
            "Precision\t0.5067764468764624\n",
            "Recall\t\t0.8472013950349153\n",
            "F1-score\t0.6171270957927629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6EP7vrK45xK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-k0ILVz45vD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp4BAe_k45pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT7e_EZq44T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u00iumtg44Id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}