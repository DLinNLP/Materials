{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tagger_solutions1-3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLImtw4oo0WV",
        "colab_type": "text"
      },
      "source": [
        "# Sequence Tagging\n",
        "---\n",
        "\n",
        "<font size=\"4\"> Tagging is the task of labelling each word in a sentence. Examples are: Part-of-Speech (POS) tagging, Named Entity Recognition (NER) and Semantic Role Labelling (SRL).\n",
        "  \n",
        "Sequence tagging can be treated as a set of independent classification tasks targeting each individual component of a sequence. However, ideally a model considers context when assigning labels to each component. \n",
        "\n",
        "Formally, the challenge in tagging is to learn a function that maps a sequence of observations x = (<i>x<sub>1</sub>,x<sub>2</sub>,....x<sub>n</sub></i>) to a label sequence y = (<i>y<sub>1</sub>,y<sub>2</sub>,...,y<sub>n</sub></i>), where <i>y<sub>i</sub></i> belongs to a set of tags, which in the case of POS tagging would be Noun, Verb, Adjective, or similar.\n",
        "\n",
        "In some tasks, a chunk in a sequence receives a label, such as <i>Cherno More</i> or <i>Black Sea</i> in \"Cherno More is the Bulgarian name of the Black Sea\". \n",
        "\n",
        "In such cases, it is a standard practice to annotate the data using the <b>IOB scheme</b>.\n",
        "\n",
        "Each element gets a label indicating whether it occurs in the beginning of chunk X (B-X), inside chunk X (I-x) or outside of any chunk (O).\n",
        "\n",
        "Common issues in tagging include:\n",
        "<ul>\n",
        "<li> Large set of features\n",
        "<li> Smaller amount of data\n",
        "<li> Expensive memory and time costs\n",
        "</ul>\n",
        "\n",
        "<br>\n",
        "  \n",
        "---\n",
        "In this notebook, we implement an <b> LSTM-based NER system</b> for CONLL2003 dataset.\n",
        "  \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm5ZYINAn6lM",
        "colab_type": "text"
      },
      "source": [
        "## Let's start!\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF8OUxU6jSkE",
        "colab_type": "code",
        "outputId": "96a110da-7015-419b-855c-d74ff5d79328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# The following two lines authorises access to Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJnwvlt7jffE",
        "colab_type": "code",
        "outputId": "8abd2ec0-98c2-4ebe-fd6f-2e1bcbbaccc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pdb\n",
        "import torch\n",
        "import gensim\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"cuda device {}available\".format(\"\" if use_cuda else \"un\"))\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Parameters\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 200\n",
        "BATCH_SIZE = 128 #300\n",
        "EPOCHS = 50\n",
        "LSTM_DROPOUT = 0.3\n",
        "\n",
        "# data dir\n",
        "data_folder = \"/content/drive/My Drive/DLinNLP/Data\"\n",
        "# embeddings folder \n",
        "embed_folder = \"/content/drive/My Drive/DLinNLP/Embeddings\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda device available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5LOm8PwpS3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec_file = embed_folder+'/vectors.txt'\n",
        "#embedding_matrix = load_pretrained(word2vec_file)\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GWyxC9mu32c",
        "colab_type": "text"
      },
      "source": [
        "## Data Format\n",
        "---\n",
        "<font size=\"4\"> One standard file format for representing tagged pieces of text is CONLL. \n",
        "  \n",
        "A CONLL file contains one token per line and an empty line indicating the end of a sentence. Each token may be annotated by several tab-separated columns indicating information about the token (e.g. token raw form) or differrent tags assigned to it (e.g. syntactic and morphological labels).\n",
        "</font>\n",
        "\n",
        "<P>\n",
        "<table align=\"left\" style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td>Welsh</td>\n",
        "    <td>NNP</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>B-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>National</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Farmers</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>'</td>\n",
        "    <td>POS</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Union</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>(</td>\n",
        "    <td>(</td> \n",
        "    <td>O</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NFU</td>\n",
        "    <td>NNP</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>B-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>)</td>\n",
        "    <td>)</td> \n",
        "    <td>O</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>chairman</td>\n",
        "    <td>NN</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>John</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>B-PER</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Lloyd</td>\n",
        "    <td>NNP</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-PER</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>said</td>\n",
        "    <td>VBD</td> \n",
        "    <td>B-VP</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>on</td>\n",
        "    <td>IN</td> \n",
        "    <td>B-PP</td>\n",
        "    <td><b>O</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>BBC</td>\n",
        "    <td>NNP</td> \n",
        "    <td>B-NP</td>\n",
        "    <td><b>B-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>radio</td>\n",
        "    <td>NN</td> \n",
        "    <td>I-NP</td>\n",
        "    <td><b>I-ORG</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>.</td>\n",
        "    <td>.</td> \n",
        "    <td>O</td>\n",
        "    <td><b>O</b></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td><br></tr>\n",
        "    <td><font size=\"4\"> Here, we read the CONLL 2003 dataset! <font></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "<br>\n",
        "</P> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpafGCXojfcB",
        "colab_type": "code",
        "outputId": "d30df5f0-dc5e-4cf3-fa16-2ff74ed19195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def readfile(filename):\n",
        "    f = open(filename)\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in f:\n",
        "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
        "            if len(sentence) > 0:\n",
        "                sentences.append(sentence)\n",
        "                sentence = []\n",
        "            continue\n",
        "        splits = line.split(' ')\n",
        "        sentence.append([splits[0].strip(), splits[-1].strip()])\n",
        "\n",
        "    if len(sentence) > 0:\n",
        "        sentences.append(sentence)\n",
        "        sentence = []\n",
        "    sentences = [tuple(zip(*l)) for l in sentences]\n",
        "    return sentences\n",
        "\n",
        "train_data = np.array(readfile(data_folder+'/train.txt'))\n",
        "dev_data = np.array(readfile(data_folder+'/dev.txt'))\n",
        "test_data = np.array(readfile(data_folder+'/test.txt'))\n",
        "\n",
        "print(\"train_data shape: \",train_data.shape)\n",
        "print(\"One instance of data: \",train_data[0])\n",
        "print(\"dev_data shape: \",dev_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data shape:  (14041, 2)\n",
            "One instance of data:  [('EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.')\n",
            " ('B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O')]\n",
            "dev_data shape:  (3250, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXW2CYLTA8zf",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing \n",
        "---\n",
        "\n",
        "<font size=\"4\">It is common to sort data instances based on their lengths. This way, the lengths of sequences in each batch would be more or less homogeneous. \n",
        "<font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL_zgRz6jfYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This reordering is useful for padding\n",
        "def tensor_reorder(data):\n",
        "    \"\"\"reorders tensors from longest to shortest\"\"\"\n",
        "    lengths = [len(i[0]) for i in data]\n",
        "    max_len = max(lengths)\n",
        "    lengths = torch.LongTensor(lengths)\n",
        "    lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "    data = data[perm_idx]\n",
        "    return data\n",
        "\n",
        "train_data = tensor_reorder(train_data)\n",
        "dev_data = tensor_reorder(dev_data)\n",
        "test_data = tensor_reorder(test_data)\n",
        "\n",
        "MAX_LEN = max(len(train_data[0][0]), len(dev_data[0][0])) # we set the maximum length from the max seq in train "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wMKVnTrjfWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here, we create a dictionary that maps all words to indices (for encoding)\n",
        "\n",
        "all_words = list(set([w for sent in np.concatenate((train_data,dev_data), axis=0) for w in sent[0]]))\n",
        "\n",
        "word_to_ix = {t:i+2 for i, t in enumerate(all_words)}\n",
        "\n",
        "word_to_ix['<PAD>'] = 0\n",
        "word_to_ix['<UNK>'] = 1\n",
        "\n",
        "# The tagset is simplified (NE categories not included) \n",
        "tag_to_ix = {'<PAD>':0, 'B-MISC':1, 'B-LOC':1, 'B-ORG':1, 'B-PER':1,\n",
        "             'I-MISC':2, 'I-PER':2, 'I-ORG':2, 'I-LOC':2, 'O':3}\n",
        "ix_to_tag = {0:'<PAD>', 1:'B', 2:'I', 3:'O'}\n",
        "\n",
        "## If we wanted to consider tags in their entirety:\n",
        "# all_tags = list(set([tag for sent_tag in train_data for tag in sent_tag[1]]))\n",
        "# tag_to_ix = {t:i+1 for i, t in enumerate(all_tags)}\n",
        "# tag_to_ix['<PAD>'] = 0\n",
        "# ix_to_tag = {v: k for k, v in tag_to_ix.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WZuAnmVilza",
        "colab_type": "text"
      },
      "source": [
        "## Dataset class and Dataloading\n",
        "---\n",
        "\n",
        "<font size=\"4\">The standard way to represent a dataset in PyTorch is through the `utils.data.Dataset` class. In order to define your dataset, your custom class should inherit from `Dataset` and override the following two methods:\n",
        "    \n",
        "1. `__len__` which returns the size of the dataset  \n",
        "2. `__getitem__` which receives an index `i`, returning the representation for the i*th* sample in the dataset  \n",
        "\n",
        "You can define other arbitrary methods depending on your specific requirements.         \n",
        "    \n",
        "<font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ5P-gPUjhRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class representa the dataset.\n",
        "class CoNLL2003NER(Dataset):\n",
        "\n",
        "    def __init__(self, X, max_len, word_to_ix, tag_to_ix):\n",
        "        self.X = X\n",
        "        self.max_len = max_len\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        \n",
        "    def transform(self, seq, to_ix):\n",
        "        \"\"\"Transorm and prepare one data instance\"\"\"\n",
        "        idxs = [to_ix[w] if w in to_ix else 1 for w in seq]\n",
        "        if len(idxs) >= self.max_len:\n",
        "            # Truncating\n",
        "            idxs = idxs[:self.max_len]    \n",
        "        else:\n",
        "            # Padding\n",
        "            idxs += [0]*(self.max_len-len(seq))\n",
        "        # torch.long tensors are usually used for indexing \n",
        "        return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the length of the dataset\"\"\"\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Given an index, return its corresponding item in the dataset\"\"\"\n",
        "        return self.transform(self.X[idx][0],self.word_to_ix), \\\n",
        "               self.transform(self.X[idx][1], self.tag_to_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9DaIIs4rn4H",
        "colab_type": "text"
      },
      "source": [
        "<font size=\"4\"> In order to iterate over a `Dataset` instance, PyTorch provides the `torch.utils.data.DataLoader` which offers the following functionalities:\n",
        "\n",
        "1.   create batches of data to be fed to a network \n",
        "2.   shuffling the data instances \n",
        "3.   loading instances using the multiprocessing workers \n",
        "    <font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6kv5mDemXfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Depending on the batch size, `drop_last` might be set to `True` in order to drop the final smaller batch.\n",
        "\n",
        "params = {'batch_size': BATCH_SIZE,\n",
        "          'shuffle': False,\n",
        "          'num_workers': 6,\n",
        "          'drop_last':True}\n",
        "\n",
        "params_test = {'batch_size': 1, # batch is set to 1 not to drop any instances here\n",
        "          'shuffle': False,\n",
        "          'num_workers': 6,\n",
        "          'drop_last':True}\n",
        "\n",
        "train_data = CoNLL2003NER(train_data, MAX_LEN, word_to_ix, tag_to_ix)\n",
        "train_data_generator = DataLoader(train_data, **params)\n",
        "\n",
        "dev_data = CoNLL2003NER(dev_data, MAX_LEN, word_to_ix, tag_to_ix)\n",
        "dev_data_generator = DataLoader(dev_data, **params)\n",
        "\n",
        "test_data = CoNLL2003NER(test_data, MAX_LEN, word_to_ix, tag_to_ix)\n",
        "test_data_generator = DataLoader(test_data, **params_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWyZNaTSgNOV",
        "colab_type": "code",
        "outputId": "729d79f7-aa9a-45ae-e4da-85521024af87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "train_data.__getitem__(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 4757, 11012, 20381,  3528, 16663, 14483,  1954, 16643, 18126, 23948,\n",
              "         24750, 13725, 18917,  4248, 21781, 25140, 17692, 23409, 14216,  1506,\n",
              "         25068, 21582,  1506, 17692, 16081,   408,   407, 14509, 21414, 16493,\n",
              "         14216, 17692, 13618, 21582, 21361, 10516, 14509, 20228, 14216, 17692,\n",
              "         14509, 23094, 21582, 21361, 22118, 24443, 11767, 14216, 17692, 14509,\n",
              "          7699, 21582, 21361,  7953,  1211, 14509, 12764, 21435, 21582, 17692,\n",
              "         13618, 14216, 21361,  1035,  6856, 14216, 17692, 14509,  2213, 21582,\n",
              "         17692,  4876, 13618, 14216, 17692, 14509,  5847, 21582, 21361, 21764,\n",
              "         14509, 14819, 14216, 17692, 13618, 21582, 21361,  1890,  9196, 14216,\n",
              "         17692, 14509,  3596, 21582, 21361, 18492,  1211, 14509, 18982, 21435,\n",
              "         14216, 17692, 13618, 21582, 21361, 19429, 14509,    17, 14216, 17692,\n",
              "         13618, 21582,  6366]),\n",
              " tensor([3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXPwobRmmXdf",
        "colab_type": "code",
        "outputId": "d64f3adf-b7b8-4073-c481-b931a2fd1718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### TO DO #3 Solution ###   \n",
        "# Loading pre-trained embbeding\n",
        "# fill embedding_matrix\n",
        "USE_PRETRAINED = True\n",
        "def load_pretrained(file):\n",
        "    \"\"\"load a weight matrix from pre-trained weights\"\"\"\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(file)\n",
        "    embedding_dimension = model.vector_size \n",
        "    embedding_matrix = np.zeros((len(word_to_ix), EMBEDDING_DIM))   \n",
        "    UNKOWN = np.random.uniform(-1, 1, embedding_dimension) \n",
        "\n",
        "    for word, i in word_to_ix.items():\n",
        "        if word in model.vocab:\n",
        "            embedding_matrix[i] = model[word] \n",
        "        else:\n",
        "            embedding_matrix[i] = UNKOWN\n",
        "\n",
        "    embedding_matrix[word_to_ix['<PAD>']] = np.zeros((embedding_dimension))\n",
        "    embedding_matrix = torch.from_numpy(embedding_matrix).type(torch.FloatTensor)\n",
        "    return embedding_matrix\n",
        "\n",
        "if USE_PRETRAINED:\n",
        "    word2vec_file = embed_folder+'/vectors.txt'\n",
        "    embedding_matrix = load_pretrained(word2vec_file)\n",
        "    print('embedings loaded...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedings loaded...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P8G0y54rpr8",
        "colab_type": "text"
      },
      "source": [
        "## Define the Model\n",
        "\n",
        "<font size=\"4\"> In PyTorch, models are defined by creating classes that inherit from `torch.nn.Module`. Defining a model involves two critical steps:\n",
        "    \n",
        "\n",
        "1.   Specifying the components of the model in the `__init__` constructor\n",
        "2.   Outlining the way these components interact in the `forward` method\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gm5mWQlmXbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A simple LSTM Tagger\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, max_len):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        \n",
        "        # load weights from pre-trained vectors or with random initialization\n",
        "        if not USE_PRETRAINED:\n",
        "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
        "        else:  \n",
        "            # load weights from pre-trained vectors \n",
        "            self.word_embeddings = nn.Embedding.from_pretrained(embedding_matrix, freeze=False) \n",
        "                                                                # freeze default is True            \n",
        "        ### TO DO #4 Solution ### \n",
        "        ### Character Embedding ###\n",
        "           \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=LSTM_DROPOUT)\n",
        "        \n",
        "        ### TO DO #5 ####\n",
        "        ### bi_directional LSTM\n",
        "        ### Add other layers to the Model ###\n",
        "   \n",
        "        # The linear layer that maps from hidden space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence): \n",
        "        embeds = self.word_embeddings(sentence)     # SHAPE: [BATCH_SIZE,MAX_LEN,WORD_EMBEDDING_DIM]\n",
        "        \n",
        "        ### TO DO ####   (10 min)\n",
        "        # Add packed padded sequences to help the loss ignores computation for PAD labeld\n",
        "        \n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1) \n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBoxn3i6mXX_",
        "colab_type": "code",
        "outputId": "474fe35b-326a-4abd-eda2-d97ba09ffcdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(ix_to_tag), MAX_LEN).to(device)\n",
        "\n",
        "# The negative log likelihood loss. \n",
        "# It is useful to train a classification problem with C classes.\n",
        "loss_function = nn.NLLLoss(ignore_index=0)  \n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001) \n",
        "\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMTagger(\n",
            "  (word_embeddings): Embedding(26885, 300)\n",
            "  (lstm): LSTM(300, 200, batch_first=True, dropout=0.3)\n",
            "  (hidden2tag): Linear(in_features=200, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oG8FVrcmXMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TO DO #1 Solution ###\n",
        "# Write an early stopping procedure\n",
        "\n",
        "def early_stop(losses, patience):\n",
        "    \"\"\"stop execution if there is consecutive decline/stagnation in the loss values.\n",
        "       patience determines how quickly we take action. \n",
        "    \"\"\"\n",
        "    stop = False\n",
        "    patience += 1\n",
        "    if len(losses)>patience and min(losses[-patience:])==losses[-patience]:\n",
        "        stop = True\n",
        "    return stop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-ZspVZwFLMy",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "<font size=\"4\">In PyTorch, you have control over all the stages of the training, from iteration over the data, to adjusting the gradients, calculating the loss and backpropagation given the computed loss. \n",
        "\n",
        "The training procedure involves two for-loops. The inner loop is over the batches of the data. Here is where per-batch loss is computed and model parameters are updated using an optimizer. \n",
        "\n",
        "The inner loop is composed of the following essential steps:\n",
        "\n",
        "1. Clear the gradients. Remember that gradients accumulate over time. In each batch, they need to be reset.  \n",
        "\n",
        "2. Run forward pass and generate an output. \n",
        "\n",
        "3. Pass the output and the gold-standard training labels to the loss function to estimate the degree of deviation from the true labels.   \n",
        "\n",
        "4. Call the `backward()` method on the loss object. This will compute gradients $\\frac{\\partial loss}{\\partial x}$ for every parameter x whose `requires_grad` is set to `True`. In order words, the backward method `back propagates` gradients to each parameter.\n",
        "\n",
        "5. Perform parameter updates by calling `optimizer.step()`. The optimizer is dependent on the computed gradients to perform this operation. \n",
        "\n",
        "The outer loop repeats the inner one for the specified number of epochs. Note that the training procedure should happen in the `training` mode which is set in the beginning of the outer loop.   <font>\n",
        "  \n",
        "### Validation\n",
        "<font size=\"4\"> In each iteration of the training, we can monitor how well the model is learning, by testing it on the hold-out validation data. It is important to set the model mode to `eval` in order to avoid updating the parameters. This is to ensure the model is not overfitting on the training data or find the best parameters for the model. <br />\n",
        "\n",
        "We record all the epoch-based validation and training losses to be able to plot the changes over time.  \n",
        "<font>\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnB2nxhJmXC_",
        "colab_type": "code",
        "outputId": "667c69b8-b23a-43ec-e4d5-9fdbffd29cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def trainer(model, epochs):\n",
        "    \"\"\"train the model for the specified # of epochs\"\"\"\n",
        "    \n",
        "\n",
        "    batch_losses = []\n",
        "    valid_losses = []\n",
        "    \n",
        "    avg_train_losses = []\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    best_valid_loss = 1000\n",
        "    best_state_dict = dict()\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "                ################\n",
        "                ## train mode ##\n",
        "                ################\n",
        "        model.train() # set the model to training mode  \n",
        "        print('Epoch:', epoch+1)\n",
        "        t0 = time.time()\n",
        "        \n",
        "        n_correct, n_total = 0, 0\n",
        "        for sentences, tags in train_data_generator:\n",
        "            sentences, tags = sentences.to(device), tags.to(device)\n",
        "            \n",
        "            # clear gradients \n",
        "            model.zero_grad()\n",
        "\n",
        "            # Run forward pass\n",
        "            predictions = model(sentences)\n",
        "            \n",
        "            # compute the loss, gradients, and update the parameters\n",
        "            predictions = predictions.permute(0,2,1)       # loss presumes labels to come 2nd (hence the permute)\n",
        "            batch_loss = loss_function(predictions, tags)  # This computes average loss over all instances of the batch \n",
        "            batch_losses.append(batch_loss.item())\n",
        "            \n",
        "            # compute number of correct predictions per epoch   \n",
        "            outputs = torch.argmax(predictions, dim=1)        \n",
        "            \n",
        "            n_correct += torch.sum(outputs==tags, dtype=torch.float)\n",
        "            n_total += float(tags.size(0) * tags.size(1))  # denominator: batch_size * max_len (e.g. 100 * 52)\n",
        "            \n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        epoch_acc = n_correct/n_total\n",
        "        epoch_loss = np.average(batch_losses)\n",
        "        avg_train_losses.append(epoch_loss) # for keeping track of avg train losses\n",
        "  \n",
        "                ################\n",
        "                ## eval mode ###\n",
        "                ################\n",
        "        model.eval() # set the model to eval mode\n",
        "        for valid_sentences, valid_tags in dev_data_generator:\n",
        "            valid_sentences, valid_tags = valid_sentences.to(device), valid_tags.to(device)\n",
        "            \n",
        "            # Run forward pass. Note since we are in eval mode, we don't need to set grad to zero  \n",
        "            valid_predictions = model(valid_sentences)\n",
        "            valid_predictions = valid_predictions.permute(0,2,1)\n",
        "            # calculate the average loss \n",
        "            valid_batch_loss = loss_function(valid_predictions, valid_tags)\n",
        "            valid_losses.append(valid_batch_loss.item())\n",
        "            \n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_state_dict = model.state_dict()\n",
        "        \n",
        "        t = time.time()\n",
        "        print('epoch loss: {}\\tepoch acc: {}\\tvalid loss:{}\\ttime:{}'.format(epoch_loss, epoch_acc.item(), valid_loss, t-t0))\n",
        "        \n",
        "        ### TO DO #1 Solution ###    (10 min)\n",
        "        # Early Stopping\n",
        "        # end training if validation losses stagnate/increase \n",
        "        if early_stop(avg_valid_losses, patience=3):\n",
        "            print(\"Early stopping...\")\n",
        "            break\n",
        "        \n",
        "        ### TO DO #2 Solution ###    (5 min)\n",
        "        # Saving the best Model\n",
        "    model.load_state_dict(best_state_dict)\n",
        "    torch.save(model.state_dict(), '/content/drive/My Drive/DLinNLP/best_model_state.pkl')\n",
        "    \n",
        "    return model, avg_train_losses, avg_valid_losses \n",
        "            \n",
        "model, avg_train_losses, avg_valid_losses = trainer(model, EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "epoch loss: 2.4052562981570533\tepoch acc: 0.9191297888755798\tvalid loss:4.78363128900528\ttime:8.337628364562988\n",
            "Epoch: 2\n",
            "epoch loss: 2.345633979989301\tepoch acc: 0.9460336565971375\tvalid loss:3.46062136054039\ttime:8.106787919998169\n",
            "Epoch: 3\n",
            "epoch loss: 2.205983897050222\tepoch acc: 0.9770807027816772\tvalid loss:2.9967818440993628\ttime:8.092079162597656\n",
            "Epoch: 4\n",
            "epoch loss: 2.125455922968344\tepoch acc: 0.9833442568778992\tvalid loss:2.7623891273140906\ttime:8.084383726119995\n",
            "Epoch: 5\n",
            "epoch loss: 2.074918691690909\tepoch acc: 0.9848214983940125\tvalid loss:2.6216709421873094\ttime:8.158266067504883\n",
            "Epoch: 6\n",
            "epoch loss: 2.040332614236286\tepoch acc: 0.9853206872940063\tvalid loss:2.527826256453991\ttime:8.139852046966553\n",
            "Epoch: 7\n",
            "epoch loss: 2.015204827915169\tepoch acc: 0.985835075378418\tvalid loss:2.4608064922264643\ttime:8.103968381881714\n",
            "Epoch: 8\n",
            "epoch loss: 1.9960847996995537\tepoch acc: 0.9861845970153809\tvalid loss:2.410801376402378\ttime:8.078259229660034\n",
            "Epoch: 9\n",
            "epoch loss: 1.9810023289952317\tepoch acc: 0.9865042567253113\tvalid loss:2.37237580411964\ttime:8.13823938369751\n",
            "Epoch: 10\n",
            "epoch loss: 1.9687915916426466\tepoch acc: 0.9866488575935364\tvalid loss:2.3414935823082925\ttime:8.134414672851562\n",
            "Epoch: 11\n",
            "epoch loss: 1.95869975924815\tepoch acc: 0.9868448972702026\tvalid loss:2.316126188256524\ttime:8.159536361694336\n",
            "Epoch: 12\n",
            "epoch loss: 1.9502091622862976\tepoch acc: 0.9870110750198364\tvalid loss:2.2959392128388085\ttime:8.137018203735352\n",
            "Epoch: 13\n",
            "epoch loss: 1.9429669423987097\tepoch acc: 0.9870713353157043\tvalid loss:2.2785690651948634\ttime:8.136350870132446\n",
            "Epoch: 14\n",
            "epoch loss: 1.9367046149235174\tepoch acc: 0.9871119260787964\tvalid loss:2.263949307160718\ttime:8.122432470321655\n",
            "Epoch: 15\n",
            "epoch loss: 1.9312431597381556\tepoch acc: 0.98714679479599\tvalid loss:2.251300870378812\ttime:8.161851406097412\n",
            "Epoch: 16\n",
            "epoch loss: 1.9264259028222857\tepoch acc: 0.987288236618042\tvalid loss:2.2402088682726027\ttime:8.137883424758911\n",
            "Epoch: 17\n",
            "epoch loss: 1.9221456074153354\tepoch acc: 0.9873358011245728\tvalid loss:2.2305915339904674\ttime:8.162293910980225\n",
            "Epoch: 18\n",
            "epoch loss: 1.9183165652831213\tepoch acc: 0.9874734282493591\tvalid loss:2.221995951367749\ttime:8.166914224624634\n",
            "Epoch: 19\n",
            "epoch loss: 1.9148701283377418\tepoch acc: 0.9875425696372986\tvalid loss:2.2142928243938247\ttime:8.15422534942627\n",
            "Epoch: 20\n",
            "epoch loss: 1.911748604418351\tepoch acc: 0.9876497983932495\tvalid loss:2.2076617170274258\ttime:8.146857976913452\n",
            "Epoch: 21\n",
            "epoch loss: 1.9089098237438618\tepoch acc: 0.9876440763473511\tvalid loss:2.2016151175612495\ttime:8.159284591674805\n",
            "Epoch: 22\n",
            "epoch loss: 1.9063094566771146\tepoch acc: 0.987814724445343\tvalid loss:2.1964123026620257\ttime:8.153210878372192\n",
            "Epoch: 23\n",
            "epoch loss: 1.9039239354660467\tepoch acc: 0.9879295229911804\tvalid loss:2.191603749031606\ttime:8.14845085144043\n",
            "Epoch: 24\n",
            "epoch loss: 1.9017243948295166\tepoch acc: 0.9878813028335571\tvalid loss:2.18730512196819\ttime:8.178003072738647\n",
            "Epoch: 25\n",
            "epoch loss: 1.8996920779238053\tepoch acc: 0.9881070852279663\tvalid loss:2.183260112977028\ttime:8.167480230331421\n",
            "Epoch: 26\n",
            "epoch loss: 1.8978095741767698\tepoch acc: 0.9880836606025696\tvalid loss:2.1795588220541293\ttime:8.179340839385986\n",
            "Epoch: 27\n",
            "epoch loss: 1.8960640830751425\tepoch acc: 0.9880392551422119\tvalid loss:2.17604131868592\ttime:8.209856033325195\n",
            "Epoch: 28\n",
            "epoch loss: 1.894435198948295\tepoch acc: 0.9881533980369568\tvalid loss:2.172951646362032\ttime:8.20685338973999\n",
            "Epoch: 29\n",
            "epoch loss: 1.8929131997514095\tepoch acc: 0.9882676005363464\tvalid loss:2.170145358546027\ttime:8.189173460006714\n",
            "Epoch: 30\n",
            "epoch loss: 1.891489040518937\tepoch acc: 0.9881908297538757\tvalid loss:2.167442381878694\ttime:8.160902738571167\n",
            "Epoch: 31\n",
            "epoch loss: 1.8901546344527058\tepoch acc: 0.9881958961486816\tvalid loss:2.1650904985589365\ttime:8.17834186553955\n",
            "Epoch: 32\n",
            "epoch loss: 1.8889076113572703\tepoch acc: 0.9880449175834656\tvalid loss:2.163048212788999\ttime:8.167811870574951\n",
            "Epoch: 33\n",
            "epoch loss: 1.8877239378750705\tepoch acc: 0.988120436668396\tvalid loss:2.161090852961396\ttime:8.140943050384521\n",
            "Epoch: 34\n",
            "epoch loss: 1.8866059142237763\tepoch acc: 0.9880690574645996\tvalid loss:2.159288963728091\ttime:8.154361248016357\n",
            "Epoch: 35\n",
            "epoch loss: 1.8855462399371172\tepoch acc: 0.9882396459579468\tvalid loss:2.1573949251685822\ttime:8.135304927825928\n",
            "Epoch: 36\n",
            "epoch loss: 1.8845402097555446\tepoch acc: 0.9883728623390198\tvalid loss:2.1557443076206577\ttime:8.144225597381592\n",
            "Epoch: 37\n",
            "epoch loss: 1.883586706279112\tepoch acc: 0.988473117351532\tvalid loss:2.1542650172839295\ttime:8.149776458740234\n",
            "Epoch: 38\n",
            "epoch loss: 1.8826822720752252\tepoch acc: 0.9885891675949097\tvalid loss:2.152921779045933\ttime:8.157529354095459\n",
            "Epoch: 39\n",
            "epoch loss: 1.881823601881649\tepoch acc: 0.9886139035224915\tvalid loss:2.1517300955913004\ttime:8.187628984451294\n",
            "Epoch: 40\n",
            "epoch loss: 1.8810074791260951\tepoch acc: 0.9886062741279602\tvalid loss:2.150623961046338\ttime:8.167221784591675\n",
            "Epoch: 41\n",
            "epoch loss: 1.8802321470538677\tepoch acc: 0.9885841012001038\tvalid loss:2.1495234154782645\ttime:8.164525508880615\n",
            "Epoch: 42\n",
            "epoch loss: 1.879489823970773\tepoch acc: 0.9885644316673279\tvalid loss:2.1484018176794053\ttime:8.15720272064209\n",
            "Epoch: 43\n",
            "epoch loss: 1.878781279583577\tepoch acc: 0.9886468648910522\tvalid loss:2.147654248919598\ttime:8.164710760116577\n",
            "Epoch: 44\n",
            "epoch loss: 1.878104508862993\tepoch acc: 0.9886595606803894\tvalid loss:2.1467367252301086\ttime:8.133960247039795\n",
            "Epoch: 45\n",
            "epoch loss: 1.877459669385056\tepoch acc: 0.9888029098510742\tvalid loss:2.145943462252617\ttime:8.145607233047485\n",
            "Epoch: 46\n",
            "epoch loss: 1.8768447787906954\tepoch acc: 0.9883652329444885\tvalid loss:2.1450963562338248\ttime:8.193331241607666\n",
            "Epoch: 47\n",
            "epoch loss: 1.8762517206700966\tepoch acc: 0.9884832501411438\tvalid loss:2.1444218405764155\ttime:8.15293836593628\n",
            "Epoch: 48\n",
            "epoch loss: 1.8756837617286939\tepoch acc: 0.9884337782859802\tvalid loss:2.1434636930003763\ttime:8.159290790557861\n",
            "Epoch: 49\n",
            "epoch loss: 1.8751355222625314\tepoch acc: 0.9885815382003784\tvalid loss:2.142801696402686\ttime:8.167442798614502\n",
            "Epoch: 50\n",
            "epoch loss: 1.8746095580860562\tepoch acc: 0.9884293079376221\tvalid loss:2.142109948158264\ttime:8.131519556045532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa4vzoEReMeB",
        "colab_type": "code",
        "outputId": "8c92153e-6cc9-4a5c-9367-2d251988e41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "# Visualizing the loss as the network trained\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "\n",
        "plt.plot(range(1,len(avg_train_losses)+1),avg_train_losses, label='Training Loss')\n",
        "plt.plot(range(1,len(avg_valid_losses)+1),avg_valid_losses,label='Validation Loss')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = avg_valid_losses.index(min(avg_valid_losses))+1 \n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAF3CAYAAAC8MNLCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B4YGlBUVEF-",
        "colab_type": "text"
      },
      "source": [
        "## Test\n",
        "<font size=\"4\">\n",
        "  Now you can compute the performance of the model on the unseen test data.\n",
        "  \n",
        "  </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9XRK1GJSjBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define evaluation metrics (acc, precision, recall, and f1-score)\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "class Evaluate():\n",
        "    def __init__(self, out, labels):\n",
        "        self.out = out.numpy().flatten()\n",
        "        self.labels = labels.numpy().flatten()\n",
        "    def accuracy(self):\n",
        "        nb_correct = sum(y_t==y_p for y_t, y_p in zip(self.labels, self.out))\n",
        "        nb_true = len(self.labels)\n",
        "        score = nb_correct / nb_true\n",
        "        return score\n",
        "    def precision_recall_fscore(self, tag_list, average='macro'):\n",
        "        return precision_recall_fscore_support(self.labels, self.out, \n",
        "                                                  average=average,labels=tag_list)[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tWDZVOimW6L",
        "colab_type": "code",
        "outputId": "0cbd5395-7818-4967-8859-3d4b1aa0a531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# evaluate the model on test set \n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/DLinNLP/best_model_state.pkl'))\n",
        "\n",
        "all_preds = torch.LongTensor().to(device)\n",
        "all_labels = torch.LongTensor().to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sent, target in test_data_generator:\n",
        "        sent, target = sent.to(device), target.to(device)\n",
        "        tag_scores = model(sent)\n",
        "        \n",
        "        predict = tag_scores.data.max(2, keepdim=True)[1]        \n",
        "        all_preds = torch.cat([all_preds, predict])\n",
        "        all_labels = torch.cat([all_labels,target])\n",
        "    \n",
        "    all_preds, all_labels = all_preds.cpu(), all_labels.cpu()\n",
        "    all_preds = all_preds.squeeze(dim=-1)\n",
        "    evaluator = Evaluate(all_preds, all_labels)\n",
        "    print('Overall Results on the Test set:')\n",
        "    print('Accuracy\\t{}'.format(evaluator.accuracy()))\n",
        "    pr, rc, fm = evaluator.precision_recall_fscore(tag_list=[1,2,3]) # we ignore pad  \n",
        "    print('Precision\\t{}\\nRecall\\t\\t{}\\nF1-score\\t{}'.format(pr,rc,fm))\n",
        "    \n",
        "    print('\\n==================\\n')\n",
        "    \n",
        "    print('# Results ignoring PAD and O:\\n')\n",
        "    tag_list = [1,2] # we ignore both pad and O\n",
        "    pr, rc, fm = evaluator.precision_recall_fscore(tag_list)\n",
        "    print('Precision\\t{}\\nRecall\\t\\t{}\\nF1-score\\t{}'.format(pr,rc,fm))         "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overall Results on the Test set:\n",
            "Accuracy\t0.9454008185776637\n",
            "Precision\t0.3377232197464391\n",
            "Recall\t\t0.33057357575470075\n",
            "F1-score\t0.318531044728471\n",
            "\n",
            "==================\n",
            "\n",
            "# Results ignoring PAD and O:\n",
            "\n",
            "Precision\t0.08872744142154093\n",
            "Recall\t\t0.17835592684324222\n",
            "F1-score\t0.11696311017442079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6EP7vrK45xK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-k0ILVz45vD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp4BAe_k45pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT7e_EZq44T1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u00iumtg44Id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}